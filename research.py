#!/usr/bin/env python3
"""
ç»Ÿä¸€æ·±åº¦ç ”ç©¶è„šæœ¬ - æ”¯æŒå®Œæ•´LangGraphå·¥ä½œæµ
åŠŸèƒ½ç‰¹æ€§ï¼š
- å¯é…ç½®æ¨¡å‹ (Qwen/DeepSeekå…¨ç³»åˆ—)
- äº¤äº’å¼æ¾„æ¸… (éµå¾ªLangGraphæ¨¡å¼)
- MCPæœ¬åœ°æ–‡æ¡£è¯»å–
- å¯å¼€å…³äº’è”ç½‘æœç´¢
- æµå¼è¿›åº¦æ˜¾ç¤º

ä½¿ç”¨æ–¹æ³•: python research.py "ä½ çš„ç ”ç©¶é—®é¢˜"
"""

import asyncio
import argparse
import uuid
import os
import sys
from typing import Optional
from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver
from open_deep_research.deep_researcher import deep_researcher
from open_deep_research.configuration import Configuration

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(".env")

class ResearchConfig:
    """ç ”ç©¶é…ç½®ç±»ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""

    def __init__(self,
                 model: str,
                 max_tokens: int,
                 search_enabled: bool = True,
                 search_api: str = "tavily",
                 allow_clarification: bool = True,
                 docs_path: Optional[str] = None,
                 max_concurrent_units: int = 8,
                 max_iterations: int = 10):

        self.model = model
        self.max_tokens = max_tokens
        self.search_enabled = search_enabled
        self.search_api = search_api if search_enabled else "none"
        self.allow_clarification = allow_clarification
        self.docs_path = docs_path
        self.max_concurrent_units = max_concurrent_units
        self.max_iterations = max_iterations

    def get_langgraph_config(self) -> dict:
        """è·å–LangGraphé…ç½®"""
        config = {
            "configurable": {
                "thread_id": str(uuid.uuid4()),

                # åŸºç¡€é…ç½®
                "max_structured_output_retries": 3,
                "allow_clarification": self.allow_clarification,
                "max_concurrent_research_units": self.max_concurrent_units,
                "search_api": self.search_api,
                "max_researcher_iterations": self.max_iterations,
                "max_react_tool_calls": 20,

                # æ¨¡å‹é…ç½® - ç»Ÿä¸€ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å’Œtokens
                "summarization_model": self.model,
                "summarization_model_max_tokens": self.max_tokens,
                "research_model": self.model,
                "research_model_max_tokens": self.max_tokens,
                "compression_model": self.model,
                "compression_model_max_tokens": self.max_tokens,
                "final_report_model": self.model,
                "final_report_model_max_tokens": self.max_tokens,
            }
        }

        # é…ç½®MCPæœ¬åœ°æ–‡æ¡£æ”¯æŒ
        if self.docs_path and os.path.exists(self.docs_path):
            config["configurable"]["mcp_config"] = {
                "transport": "stdio",
                "command": "npx",
                "args": [
                    "-y",
                    "@modelcontextprotocol/server-filesystem",
                    os.path.abspath(self.docs_path)
                ],
                "tools": ["read_text_file", "list_directory", "read_file"],
                "auth_required": False
            }
            config["configurable"]["mcp_prompt"] = (
                f"ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹MCPå·¥å…·è®¿é—®æœ¬åœ°æ–‡æ¡£ï¼š\n"
                f"- read_text_file: è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹\n"
                f"- list_directory: åˆ—å‡ºç›®å½•å†…å®¹\n"
                f"- read_file: è¯»å–ä»»æ„æ–‡ä»¶\n"
                f"ç›®å½•è·¯å¾„: {self.docs_path}\n"
                f"è¯·ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡æ¡£ä¿¡æ¯ï¼Œå‡å°‘å¹»è§‰ï¼Œæä¾›å‡†ç¡®çš„ç ”ç©¶ç»“æœã€‚"
            )

        return config

    def validate(self) -> bool:
        """éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§"""
        # éªŒè¯æ¨¡å‹æ ¼å¼ - ä½¿ç”¨æ¨ªæ æ ¼å¼
        valid_models = [
            "qwen-flash", "qwen-plus", "deepseek-chat", "deepseek-reasoner"
        ]
        if self.model not in valid_models:
            print(f"âŒ æ— æ•ˆçš„æ¨¡å‹åç§°: {self.model}")
            print(f"âœ… æ”¯æŒçš„æ¨¡å‹: {', '.join(valid_models)}")
            return False

        # éªŒè¯æ–‡æ¡£è·¯å¾„
        if self.docs_path and not os.path.exists(self.docs_path):
            print(f"âŒ æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨: {self.docs_path}")
            return False

        # éªŒè¯æœç´¢APIé…ç½®
        if self.search_enabled and self.search_api == "tavily":
            if not os.getenv("TAVILY_API_KEY"):
                print("âš ï¸  è­¦å‘Š: å¯ç”¨äº†Tavilyæœç´¢ä½†æœªè®¾ç½®TAVILY_API_KEY")

        return True

    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("ğŸ”§ ç ”ç©¶é…ç½®:")
        print(f"   æ¨¡å‹: {self.model}")
        print(f"   æœ€å¤§Tokens: {self.max_tokens}")
        print(f"   äº’è”ç½‘æœç´¢: {'âœ… å¼€å¯' if self.search_enabled else 'âŒ å…³é—­'}")
        if self.search_enabled:
            print(f"   æœç´¢å¼•æ“: {self.search_api}")
        print(f"   äº¤äº’æ¾„æ¸…: {'âœ… å¼€å¯' if self.allow_clarification else 'âŒ å…³é—­'}")
        if self.docs_path:
            print(f"   æœ¬åœ°æ–‡æ¡£: {self.docs_path}")
        print(f"   å¹¶å‘æ•°é‡: {self.max_concurrent_units}")
        print(f"   æœ€å¤§è½®æ¬¡: {self.max_iterations}")
        print("-" * 50)


def select_documents_interactive() -> Optional[str]:
    """äº¤äº’å¼é€‰æ‹©æ–‡æ¡£è·¯å¾„"""
    print("\nğŸ“ é€‰æ‹©è¦ç ”ç©¶çš„æ–‡æ¡£:")
    print("=" * 50)

    # æä¾›é¢„è®¾é€‰é¡¹
    options = [
        ("./test_docs", "æµ‹è¯•æ–‡æ¡£ç›®å½•"),
        ("./src", "æºä»£ç ç›®å½•"),
        ("./", "é¡¹ç›®æ ¹ç›®å½•"),
        ("custom", "è‡ªå®šä¹‰è·¯å¾„"),
        ("none", "ä¸ä½¿ç”¨æœ¬åœ°æ–‡æ¡£")
    ]

    for i, (path, desc) in enumerate(options, 1):
        status = ""
        if path != "custom" and path != "none" and os.path.exists(path):
            status = " âœ…"
        elif path != "custom" and path != "none":
            status = " âŒ"
        print(f"{i}. {desc} ({path}){status}")

    while True:
        try:
            choice = input("\nè¯·é€‰æ‹© (1-5): ").strip()

            if choice == "1" and os.path.exists("./test_docs"):
                return os.path.abspath("./test_docs")
            elif choice == "2" and os.path.exists("./src"):
                return os.path.abspath("./src")
            elif choice == "3":
                return os.path.abspath("./")
            elif choice == "4":
                custom_path = input("è¯·è¾“å…¥è‡ªå®šä¹‰è·¯å¾„: ").strip()
                if os.path.exists(custom_path):
                    return os.path.abspath(custom_path)
                else:
                    print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {custom_path}")
                    continue
            elif choice == "5":
                return None
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                continue

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return None

    return None


async def run_research(question: str, config: ResearchConfig) -> Optional[dict]:
    """è¿è¡Œæ·±åº¦ç ”ç©¶æµç¨‹"""

    # éªŒè¯é…ç½®
    if not config.validate():
        return None

    # æ‰“å°é…ç½®æ‘˜è¦
    config.print_summary()

    print(f"ğŸ” å¼€å§‹ç ”ç©¶: {question}")
    print("=" * 50)

    try:
        # åˆå§‹åŒ–LangGraphå·¥ä½œæµ - deep_researcher å·²ç»æ˜¯ç¼–è¯‘å¥½çš„å›¾
        graph = deep_researcher

        # è·å–é…ç½®
        langgraph_config = config.get_langgraph_config()

        # æ‰§è¡Œç ”ç©¶æµç¨‹ï¼Œæ”¯æŒæµå¼è¾“å‡º
        step_count = 0
        final_result = None
        current_stage = "åˆå§‹åŒ–"

        async for event in graph.astream(
            {"messages": [{"role": "user", "content": question}]},
            langgraph_config,
            stream_mode="updates"
        ):
            step_count += 1

            for node_name, node_state in event.items():
                # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ node_state ä¸ä¸º None
                if node_state is None:
                    continue

                # æ›´æ–°é˜¶æ®µæ˜¾ç¤º
                stage_map = {
                    "clarify_with_user": "ğŸ’¬ æ¾„æ¸…é˜¶æ®µ",
                    "write_research_brief": "ğŸ“ è®¡åˆ’é˜¶æ®µ",
                    "research_supervisor": "ğŸ”¬ ç ”ç©¶é˜¶æ®µ",
                    "final_report_generation": "ğŸ“„ æŠ¥å‘Šé˜¶æ®µ"
                }
                current_stage = stage_map.get(node_name, node_name)

                print(f"\n[{step_count}] {current_stage}")

                # æ¾„æ¸…é˜¶æ®µå¤„ç†
                if node_name == "clarify_with_user":
                    if isinstance(node_state, dict) and "messages" in node_state and node_state["messages"]:
                        latest_message = node_state["messages"][-1]
                        if hasattr(latest_message, 'content'):
                            message_content = latest_message.content

                            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ·æ¾„æ¸…
                            if len(message_content) > 50 and ("?" in message_content or "æ¾„æ¸…" in message_content):
                                print(f"\nğŸ¤” ç³»ç»Ÿè¯¢é—®: {message_content}")
                                # è¿™é‡ŒLangGraphä¼šè‡ªåŠ¨ç­‰å¾…ç”¨æˆ·è¾“å…¥å¹¶ç»§ç»­æµç¨‹

                # ç ”ç©¶è®¡åˆ’é˜¶æ®µ
                elif node_name == "write_research_brief":
                    if isinstance(node_state, dict) and "research_brief" in node_state:
                        brief = node_state.get('research_brief', '')[:200] + "..."
                        print(f"ğŸ“‹ ç ”ç©¶è®¡åˆ’: {brief}")

                # ç ”ç©¶æ‰§è¡Œé˜¶æ®µ
                elif node_name == "research_supervisor":
                    if isinstance(node_state, dict) and "notes" in node_state and node_state["notes"]:
                        notes_count = len(node_state["notes"])
                        print(f"ğŸ“š å·²æ”¶é›† {notes_count} æ¡ç ”ç©¶èµ„æ–™")

                        # æ˜¾ç¤ºæœ€æ–°ç ”ç©¶å†…å®¹é¢„è§ˆ
                        if notes_count > 0:
                            latest_note = node_state["notes"][-1]
                            preview = latest_note[:150] + "..." if len(latest_note) > 150 else latest_note
                            print(f"ğŸ” æœ€æ–°å‘ç°: {preview}")

                # æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ
                elif node_name == "final_report_generation":
                    print("âœï¸  æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š...")
                    if isinstance(node_state, dict) and "final_report" in node_state:
                        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")

            final_result = node_state if len(event) == 1 else event

        # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
        if final_result:
            print("\n" + "=" * 60)
            print("ğŸ“Š æ·±åº¦ç ”ç©¶æŠ¥å‘Š")
            print("=" * 60)

            # å¤„ç†ä¸åŒç±»å‹çš„ final_result
            if isinstance(final_result, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å– final_report
                report = final_result.get("final_report", "âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            else:
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œå¯èƒ½æ˜¯äº‹ä»¶å­—å…¸
                report = "âŒ æ— æ³•è§£ææŠ¥å‘Šæ ¼å¼"
                # å°è¯•ä»eventä¸­è§£æ
                for node_name, node_state in final_result.items():
                    if node_name == "final_report_generation" and isinstance(node_state, dict):
                        report = node_state.get("final_report", "âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                        break

            # å¤„ç†ä¸åŒçš„æŠ¥å‘Šæ ¼å¼
            if hasattr(report, 'content'):
                report_text = report.content
            elif isinstance(report, str):
                report_text = report
            else:
                report_text = str(report)

            print(report_text)
            print("\n" + "=" * 60)

        return final_result

    except Exception as e:
        print(f"âŒ ç ”ç©¶è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•° - å¤„ç†å‘½ä»¤è¡Œå‚æ•°å’Œç”¨æˆ·äº¤äº’"""
    parser = argparse.ArgumentParser(
        description="ç»Ÿä¸€æ·±åº¦ç ”ç©¶å·¥å…· - æ”¯æŒå®Œæ•´LangGraphå·¥ä½œæµ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ”¯æŒçš„æ¨¡å‹:
  qwen-flash         - Qwen Flash (å¿«é€Ÿ)
  qwen-plus          - Qwen Plus (å‡è¡¡)
  deepseek-chat      - DeepSeek Chat (å¯¹è¯)
  deepseek-reasoner  - DeepSeek Reasoning (æ¨ç†)

ä½¿ç”¨ç¤ºä¾‹:
  python research.py "AIçš„å‘å±•å†ç¨‹" --model deepseek-reasoner --max-tokens 8192
  python research.py "æœºå™¨å­¦ä¹ ç®—æ³•æ¯”è¾ƒ" --model qwen-plus --max-tokens 4096
  python research.py "æœ¬åœ°é¡¹ç›®åˆ†æ" --docs-path ./src --no-search --model deepseek-chat --max-tokens 8192
  python research.py "å¿«é€ŸæŸ¥è¯¢" --no-clarify --model qwen-flash --max-tokens 2048
        """
    )

    parser.add_argument("question", help="ç ”ç©¶é—®é¢˜æˆ–ä¸»é¢˜")
    parser.add_argument("--model", required=True,
                       help="ä½¿ç”¨çš„æ¨¡å‹ (å¿…éœ€å‚æ•°)")
    parser.add_argument("--no-search", action="store_true",
                       help="ç¦ç”¨äº’è”ç½‘æœç´¢ï¼Œä»…ä½¿ç”¨æœ¬åœ°æ–‡æ¡£")
    parser.add_argument("--search-api", default="tavily", choices=["tavily", "none"],
                       help="æœç´¢å¼•æ“é€‰æ‹© (é»˜è®¤: tavily)")
    parser.add_argument("--no-clarify", action="store_true",
                       help="è·³è¿‡äº¤äº’å¼æ¾„æ¸…ï¼Œç›´æ¥å¼€å§‹ç ”ç©¶")
    parser.add_argument("--docs-path",
                       help="æŒ‡å®šæœ¬åœ°æ–‡æ¡£è·¯å¾„")
    parser.add_argument("--interactive-docs", action="store_true",
                       help="äº¤äº’å¼é€‰æ‹©æ–‡æ¡£è·¯å¾„")
    parser.add_argument("--max-concurrent", type=int, default=8,
                       help="æœ€å¤§å¹¶å‘ç ”ç©¶å•å…ƒæ•° (é»˜è®¤: 8ï¼Œç”¨äºæ·±åº¦ç ”ç©¶)")
    parser.add_argument("--max-iterations", type=int, default=10,
                       help="æœ€å¤§ç ”ç©¶è½®æ¬¡ (é»˜è®¤: 10ï¼Œç”¨äºæ·±åº¦ç ”ç©¶)")
    parser.add_argument("--max-tokens", type=int, required=True,
                       help="æ¨¡å‹æœ€å¤§tokenæ•° (å¿…éœ€å‚æ•°)")

    args = parser.parse_args()

    # äº¤äº’å¼é€‰æ‹©æ–‡æ¡£è·¯å¾„
    docs_path = args.docs_path
    if args.interactive_docs:
        docs_path = select_documents_interactive()

    # åˆ›å»ºç ”ç©¶é…ç½®
    config = ResearchConfig(
        model=args.model,
        max_tokens=args.max_tokens,
        search_enabled=not args.no_search,
        search_api=args.search_api,
        allow_clarification=not args.no_clarify,
        docs_path=docs_path,
        max_concurrent_units=args.max_concurrent,
        max_iterations=args.max_iterations
    )

    # è¿è¡Œç ”ç©¶
    print("ğŸš€ å¯åŠ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿ...")
    result = asyncio.run(run_research(args.question, config))

    if result:
        print("\nâœ… ç ”ç©¶å®Œæˆ!")
        return 0
    else:
        print("\nâŒ ç ”ç©¶å¤±è´¥!")
        return 1


if __name__ == "__main__":
    sys.exit(main())