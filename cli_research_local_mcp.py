#!/usr/bin/env python3
"""
åŸºäºMCPçš„æœ¬åœ°æ–‡ä»¶ç ”ç©¶è„šæœ¬
ä½¿ç”¨æ–¹æ³•: python cli_research_local_mcp.py "ç ”ç©¶é—®é¢˜" --docs-path "/path/to/docs"
"""

import os
import argparse
import asyncio
import uuid
from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from open_deep_research.deep_researcher import (
    write_research_brief,
    supervisor_subgraph,
    final_report_generation
)
from open_deep_research.state import AgentState, AgentInputState
from open_deep_research.configuration import Configuration

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(".env")

async def run_mcp_local_research(question: str, docs_path: str, model: str = "deepseek:deepseek-reasoner"):
    """åŸºäºMCPçš„æœ¬åœ°æ–‡ä»¶ç ”ç©¶"""

    # éªŒè¯æ–‡æ¡£è·¯å¾„
    if not os.path.exists(docs_path):
        print(f"é”™è¯¯: æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨: {docs_path}")
        return None

    abs_docs_path = os.path.abspath(docs_path)
    print(f"ç ”ç©¶é—®é¢˜: {question}")
    print(f"æ–‡æ¡£è·¯å¾„: {abs_docs_path}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model}")
    print("=" * 60)

    # æ˜¾ç¤ºå¯ç”¨æ–‡ä»¶
    print("æ‰«æå¯ç”¨æ–‡ä»¶...")
    file_count = 0
    for root, dirs, files in os.walk(abs_docs_path):
        for file in files:
            if file.endswith(('.md', '.csv', '.txt', '.py', '.json', '.yaml', '.yml')):
                rel_path = os.path.relpath(os.path.join(root, file), abs_docs_path)
                print(f"  ğŸ“„ {rel_path}")
                file_count += 1

    print(f"å…±å‘ç° {file_count} ä¸ªå¯è¯»å–æ–‡ä»¶")
    print("=" * 60)

    # åˆ›å»ºç ”ç©¶å›¾
    builder = StateGraph(
        AgentState,
        input=AgentInputState,
        config_schema=Configuration
    )

    # æ·»åŠ èŠ‚ç‚¹ï¼ˆè·³è¿‡æ¾„æ¸…ï¼Œä¸“æ³¨æœ¬åœ°æ–‡ä»¶ç ”ç©¶ï¼‰
    builder.add_node("write_research_brief", write_research_brief)
    builder.add_node("research_supervisor", supervisor_subgraph)
    builder.add_node("final_report_generation", final_report_generation)

    # å®šä¹‰è¾¹
    builder.add_edge(START, "write_research_brief")
    builder.add_edge("write_research_brief", "research_supervisor")
    builder.add_edge("research_supervisor", "final_report_generation")
    builder.add_edge("final_report_generation", END)

    # ç¼–è¯‘å›¾
    graph = builder.compile(checkpointer=MemorySaver())

    # é…ç½®MCPæ–‡ä»¶ç³»ç»ŸæœåŠ¡å™¨
    config = {
        "configurable": {
            "thread_id": str(uuid.uuid4()),
            # åŸºæœ¬é…ç½®
            "max_structured_output_retries": 3,
            "allow_clarification": False,
            "max_concurrent_research_units": 2,  # å‡å°‘å¹¶å‘ï¼Œä¸“æ³¨æœ¬åœ°æ–‡ä»¶
            "search_api": "none",  # ç¦ç”¨ç½‘ç»œæœç´¢ï¼Œä¸“æ³¨æœ¬åœ°æ–‡ä»¶
            "disable_web_search": True,  # å¼ºåˆ¶ç¦ç”¨æ‰€æœ‰ç½‘ç»œæœç´¢
            "max_researcher_iterations": 4,     # å‡å°‘è¿­ä»£æ¬¡æ•°
            "max_react_tool_calls": 8,          # å‡å°‘å·¥å…·è°ƒç”¨æ¬¡æ•°

            # æ¨¡å‹é…ç½®
            "summarization_model": "deepseek:deepseek-chat",
            "summarization_model_max_tokens": 4096,
            "research_model": model,
            "research_model_max_tokens": 8192,
            "compression_model": "deepseek:deepseek-chat",
            "compression_model_max_tokens": 4096,
            "final_report_model": model,
            "final_report_model_max_tokens": 8192,

            # MCPæ–‡ä»¶ç³»ç»Ÿé…ç½®
            "mcp_config": {
                "url": f"stdio://npx @modelcontextprotocol/server-filesystem {abs_docs_path}",
                "tools": ["read_text_file", "list_directory", "search_files"],
                "auth_required": False
            },
            "mcp_prompt": f"""
ğŸš« é‡è¦ï¼šä½ è¢«é…ç½®ä¸ºä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è¿›è¡Œç ”ç©¶ï¼Œä¸¥ç¦ä½¿ç”¨ä»»ä½•ç½‘ç»œæœç´¢æˆ–åœ¨çº¿èµ„æºï¼

ğŸ“ ä½ ç°åœ¨å¯ä»¥è®¿é—® {abs_docs_path} ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚

ğŸ› ï¸ å¯ç”¨MCPå·¥å…·ï¼š
- read_text_file: è¯»å–æŒ‡å®šæ–‡ä»¶å†…å®¹
- list_directory: æŸ¥çœ‹ç›®å½•å†…å®¹
- search_files: åœ¨ç›®å½•ä¸­æœç´¢æ–‡ä»¶

ğŸ“‹ ç ”ç©¶è¦æ±‚ï¼š
1. å¿…é¡»ä½¿ç”¨read_text_fileå·¥å…·è¯»å–æœ¬åœ°æ–‡ä»¶
2. æ‰€æœ‰ä¿¡æ¯å¿…é¡»æ¥æºäºæœ¬åœ°æ–‡ä»¶
3. åœ¨å›ç­”ä¸­æ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºçš„æ–‡ä»¶å
4. å¦‚æœæœ¬åœ°æ–‡ä»¶ä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜è€Œä¸æ˜¯çŒœæµ‹

ğŸ“„ å‘ç°çš„æ–‡ä»¶ç±»å‹åŒ…æ‹¬ï¼šMarkdownæ–‡æ¡£ã€CSVæ•°æ®ã€æ–‡æœ¬æ–‡ä»¶ç­‰ã€‚

è¯·ä¸¥æ ¼åŸºäºæœ¬åœ°æ–‡ä»¶å†…å®¹è¿›è¡Œæ·±åº¦ç ”ç©¶åˆ†æï¼
"""
        }
    }

    print("å¼€å§‹æœ¬åœ°æ–‡ä»¶ç ”ç©¶...")
    print("=" * 60)

    step_count = 0
    final_result = None

    try:
        # æµå¼æ˜¾ç¤ºç ”ç©¶è¿›å±•
        async for event in graph.astream(
            {"messages": [{"role": "user", "content": question}]},
            config,
            stream_mode="updates"
        ):
            step_count += 1

            # æ˜¾ç¤ºå½“å‰èŠ‚ç‚¹ä¿¡æ¯
            for node_name, node_state in event.items():
                print(f"\n[{step_count}] æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")

                # æ£€æŸ¥æ˜¯å¦æœ‰MCPå·¥å…·è°ƒç”¨
                if "messages" in node_state:
                    for msg in node_state["messages"]:
                        if hasattr(msg, 'tool_calls') and msg.tool_calls:
                            for tool_call in msg.tool_calls:
                                if tool_call.get('name', '').startswith(('read_', 'list_')):
                                    print(f"ğŸ”§ MCPå·¥å…·è°ƒç”¨: {tool_call.get('name', '')} - {tool_call.get('args', {})}")

                # æ˜¾ç¤ºç ”ç©¶è®¡åˆ’
                if node_name == "write_research_brief":
                    if "research_brief" in node_state:
                        brief = node_state.get('research_brief', '')
                        print(f"ğŸ“‹ ç ”ç©¶è®¡åˆ’: {brief}")

                # æ˜¾ç¤ºç ”ç©¶è¿›åº¦
                elif node_name == "research_supervisor":
                    if "notes" in node_state and node_state["notes"]:
                        notes_count = len(node_state["notes"])
                        print(f"ğŸ“š å·²æ”¶é›† {notes_count} æ¡ç ”ç©¶èµ„æ–™")

                        # æ˜¾ç¤ºæœ€æ–°ç ”ç©¶å†…å®¹é¢„è§ˆ
                        if notes_count > 0:
                            latest_note = node_state["notes"][-1]
                            preview = latest_note[:200] + "..." if len(latest_note) > 200 else latest_note
                            print(f"ğŸ“– æœ€æ–°ç ”ç©¶å†…å®¹: {preview}")

                            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ¬åœ°æ–‡ä»¶å¼•ç”¨
                            if "ai_history.md" in latest_note or "ai_timeline.csv" in latest_note:
                                print("âœ… æ£€æµ‹åˆ°ä½¿ç”¨äº†æœ¬åœ°æ–‡ä»¶å†…å®¹")
                            else:
                                print("âš ï¸  æœªæ˜ç¡®æ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶ä½¿ç”¨")

                # æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ
                elif node_name == "final_report_generation":
                    print("ğŸ“ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")

            final_result = node_state if len(event) == 1 else event

        # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
        if final_result:
            print("\n" + "=" * 60)
            print("ğŸ“Š ç ”ç©¶æŠ¥å‘Š:")
            print("=" * 60)

            report = final_result.get("final_report", "æœªç”ŸæˆæŠ¥å‘Š")
            if hasattr(report, 'content'):
                report = report.content
            print(report)

            # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
            timestamp = uuid.uuid4().hex[:8]
            report_filename = f"research_report_{timestamp}.md"
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write(f"# ç ”ç©¶æŠ¥å‘Š\n\n**ç ”ç©¶é—®é¢˜**: {question}\n\n**æ–‡æ¡£è·¯å¾„**: {abs_docs_path}\n\n**ç”Ÿæˆæ—¶é—´**: {uuid.uuid4()}\n\n---\n\n{report}")

            print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_filename}")

        return final_result

    except Exception as e:
        print(f"âŒ ç ”ç©¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    parser = argparse.ArgumentParser(description="åŸºäºMCPçš„æœ¬åœ°æ–‡ä»¶ç ”ç©¶å·¥å…·")
    parser.add_argument("question", help="ç ”ç©¶é—®é¢˜æˆ–ä¸»é¢˜")
    parser.add_argument("--docs-path", required=True, help="è¦ç ”ç©¶çš„æ–‡æ¡£ç›®å½•è·¯å¾„")
    parser.add_argument("--model", default="deepseek:deepseek-reasoner", help="ä½¿ç”¨çš„æ¨¡å‹")

    args = parser.parse_args()

    # è¿è¡Œæœ¬åœ°æ–‡ä»¶ç ”ç©¶
    result = asyncio.run(run_mcp_local_research(args.question, args.docs_path, args.model))

    if result:
        print("\nâœ… ç ”ç©¶å®Œæˆ!")
    else:
        print("\nâŒ ç ”ç©¶å¤±è´¥!")

if __name__ == "__main__":
    main()