#!/usr/bin/env python3
"""
çœŸæ­£çš„MCPé›†æˆç ”ç©¶è„šæœ¬ - é›†æˆåˆ°å®é™…çš„ç ”ç©¶ç³»ç»Ÿä¸­
ä½¿ç”¨æ–¹æ³•: python cli_research_with_real_mcp.py "ç ”ç©¶é—®é¢˜" --docs-path "./test_docs"
"""

import os
import argparse
import asyncio
import uuid
from datetime import datetime
from dotenv import load_dotenv
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(".env")

async def load_mcp_tools_direct(docs_path: str):
    """ç›´æ¥åŠ è½½MCPå·¥å…·"""
    abs_docs_path = os.path.abspath(docs_path)

    # MCPé…ç½® - ä½¿ç”¨æµ‹è¯•æˆåŠŸçš„æ ¼å¼
    mcp_config = {
        "filesystem": {
            "transport": "stdio",
            "command": "npx",
            "args": ["@modelcontextprotocol/server-filesystem", abs_docs_path]
        }
    }

    # åˆ›å»ºMCPå®¢æˆ·ç«¯å¹¶è·å–å·¥å…·
    client = MultiServerMCPClient(mcp_config)
    tools = await client.get_tools()

    return tools, client

async def run_real_mcp_research(question: str, docs_path: str, model: str = "deepseek:deepseek-reasoner"):
    """è¿è¡ŒçœŸæ­£çš„MCPé›†æˆç ”ç©¶"""

    # éªŒè¯æ–‡æ¡£è·¯å¾„
    if not os.path.exists(docs_path):
        print(f"é”™è¯¯: æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨: {docs_path}")
        return None

    abs_docs_path = os.path.abspath(docs_path)
    print(f"ç ”ç©¶é—®é¢˜: {question}")
    print(f"æ–‡æ¡£è·¯å¾„: {abs_docs_path}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model}")
    print("=" * 60)

    try:
        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½MCPå·¥å…·
        print("ğŸ“¡ åŠ è½½MCPå·¥å…·...")
        tools, mcp_client = await load_mcp_tools_direct(docs_path)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(tools)} ä¸ªMCPå·¥å…·")

        # è·å–å…³é”®å·¥å…·
        list_tool = next((t for t in tools if t.name == "list_directory"), None)
        read_tool = next((t for t in tools if t.name == "read_text_file"), None)

        if not list_tool or not read_tool:
            print("âŒ ç¼ºå°‘å¿…è¦çš„MCPå·¥å…·")
            return None

        # ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–LLM
        print("ğŸ¤– åˆå§‹åŒ–ç ”ç©¶æ¨¡å‹...")
        llm = init_chat_model(model, temperature=0.1)

        # ç¬¬ä¸‰æ­¥ï¼šæ‰«æå¹¶è¯»å–æ–‡ä»¶
        print("ğŸ“ æ‰«æå¹¶è¯»å–æœ¬åœ°æ–‡ä»¶...")
        base_dir = os.path.basename(abs_docs_path)

        # åˆ—å‡ºæ–‡ä»¶
        files_result = await list_tool.ainvoke({"path": base_dir})
        print(f"å‘ç°æ–‡ä»¶: {files_result.strip()}")

        # è¯»å–æ‰€æœ‰æ–‡ä»¶
        file_contents = {}
        file_names = []
        for line in files_result.split('\n'):
            if '[FILE]' in line:
                file_name = line.split('[FILE]')[1].strip()
                file_names.append(file_name)

        for file_name in file_names:
            try:
                file_path = f"{base_dir}/{file_name}"
                print(f"  ğŸ“– è¯»å–æ–‡ä»¶: {file_path}")
                content = await read_tool.ainvoke({"path": file_path})
                file_contents[file_name] = content
                print(f"  âœ… æˆåŠŸè¯»å– {file_name} ({len(content)} å­—ç¬¦)")
            except Exception as e:
                print(f"  âŒ è¯»å– {file_name} å¤±è´¥: {e}")

        # ç¬¬å››æ­¥ï¼šæ„å»ºç ”ç©¶æç¤º
        print("ğŸ” å¼€å§‹AIåˆ†æ...")

        # æ„å»ºåŒ…å«æ‰€æœ‰æœ¬åœ°æ–‡ä»¶å†…å®¹çš„æç¤º
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æœ¬åœ°æ–‡æ¡£å†…å®¹ï¼Œé’ˆå¯¹é—®é¢˜ã€Œ{question}ã€è¿›è¡Œæ·±åº¦ç ”ç©¶å’Œåˆ†æã€‚

ğŸš« é‡è¦é™åˆ¶ï¼š
- ä½ åªèƒ½ä½¿ç”¨ä¸‹é¢æä¾›çš„æœ¬åœ°æ–‡æ¡£å†…å®¹
- ä¸¥ç¦ä½¿ç”¨ä»»ä½•ç½‘ç»œæœç´¢æˆ–å¤–éƒ¨ä¿¡æ¯
- æ‰€æœ‰åˆ†æå¿…é¡»åŸºäºæœ¬åœ°æ–‡æ¡£ä¸­çš„äº‹å®

ğŸ“ å¯ç”¨çš„æœ¬åœ°æ–‡æ¡£ï¼š

"""

        # æ·»åŠ æ‰€æœ‰æ–‡ä»¶å†…å®¹
        for filename, content in file_contents.items():
            prompt += f"""
## ğŸ“„ {filename}
```
{content}
```

"""

        prompt += f"""

ğŸ“‹ ç ”ç©¶ä»»åŠ¡ï¼š
è¯·åŸºäºä¸Šè¿°æœ¬åœ°æ–‡æ¡£å†…å®¹ï¼Œå¯¹ã€Œ{question}ã€è¿›è¡Œå…¨é¢æ·±å…¥çš„åˆ†æã€‚

è¦æ±‚ï¼š
1. ä»”ç»†åˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£å†…å®¹
2. æå–ä¸ç ”ç©¶é—®é¢˜ç›¸å…³çš„å…³é”®ä¿¡æ¯
3. è¿›è¡Œæ·±åº¦åˆ†æå’Œç»¼åˆ
4. ç”Ÿæˆç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Š
5. åœ¨å›ç­”ä¸­æ˜ç¡®å¼•ç”¨å…·ä½“çš„æ–‡æ¡£å†…å®¹
6. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºé™åˆ¶

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šã€‚"""

        # ç¬¬äº”æ­¥ï¼šæ‰§è¡ŒAIç ”ç©¶
        messages = [HumanMessage(content=prompt)]
        response = await llm.ainvoke(messages)

        # ç¬¬å…­æ­¥ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = f"""# åŸºäºæœ¬åœ°MCPæ–‡æ¡£çš„ç ”ç©¶æŠ¥å‘Š

**ç ”ç©¶é—®é¢˜**: {question}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**æ•°æ®æ¥æº**: æœ¬åœ°MCPæ–‡ä»¶ç³»ç»Ÿ ({abs_docs_path})
**åˆ†ææ¨¡å‹**: {model}

---

## ğŸ“ æ•°æ®æ¥æº

åŸºäºä»¥ä¸‹æœ¬åœ°æ–‡ä»¶è¿›è¡Œåˆ†æï¼š
{chr(10).join([f"- {name} ({len(content)} å­—ç¬¦)" for name, content in file_contents.items()])}

---

## ğŸ” AIåˆ†æç»“æœ

{response.content}

---

## ğŸ“Š æŠ€æœ¯è¯´æ˜

- âœ… ä½¿ç”¨MCP (Model Context Protocol) è¯»å–æœ¬åœ°æ–‡ä»¶
- âœ… æ‰€æœ‰æ•°æ®æ¥æºäºæœ¬åœ°æ–‡æ¡£ï¼Œç¡®ä¿éšç§å®‰å…¨
- âœ… ä½¿ç”¨ {model} æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
- âœ… å®Œå…¨ç¦»çº¿åˆ†æï¼Œæ— ç½‘ç»œæœç´¢

*æœ¬æŠ¥å‘Šç”±MCPé›†æˆç ”ç©¶ç³»ç»Ÿç”Ÿæˆ*
"""

        # ç¬¬ä¸ƒæ­¥ï¼šä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"mcp_ai_research_{timestamp}.md"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(final_report)

        print("\nğŸ“Š AIç ”ç©¶æŠ¥å‘Š:")
        print("=" * 60)
        print(final_report)
        print("=" * 60)
        print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_filename}")

        return final_report

    except Exception as e:
        print(f"âŒ ç ”ç©¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        if 'mcp_client' in locals():
            try:
                # æ¸…ç†MCPå®¢æˆ·ç«¯
                print("ğŸ”š æ¸…ç†MCPè¿æ¥...")
            except:
                pass

def main():
    parser = argparse.ArgumentParser(description="çœŸæ­£çš„MCPé›†æˆç ”ç©¶å·¥å…·")
    parser.add_argument("question", help="ç ”ç©¶é—®é¢˜æˆ–ä¸»é¢˜")
    parser.add_argument("--docs-path", default="./test_docs", help="è¦ç ”ç©¶çš„æ–‡æ¡£ç›®å½•è·¯å¾„")
    parser.add_argument("--model", default="deepseek:deepseek-reasoner", help="ä½¿ç”¨çš„AIæ¨¡å‹")

    args = parser.parse_args()

    # è¿è¡ŒçœŸæ­£çš„MCPç ”ç©¶
    result = asyncio.run(run_real_mcp_research(args.question, args.docs_path, args.model))

    if result:
        print("\nâœ… ç ”ç©¶å®Œæˆ!")
    else:
        print("\nâŒ ç ”ç©¶å¤±è´¥!")

if __name__ == "__main__":
    main()